{"input":"Tokenization is the process of breaking text into smaller units called tokens. These tokens can be words, subwords, or characters, and the choice of strategy affects how well a model can handle different languages, rare words, and misspellings.","output":"Tokenization splits text into smaller units like words, subwords or characters, and the chosen method strongly affects how well a model handles languages, rare words and spelling variations."}
{"input":"Self-attention is a mechanism used in transformer models that lets each position in a sequence look at other positions to decide which parts of the input are most relevant. By computing attention weights between tokens, the model can capture long-range dependencies without using recurrence.","output":"Self-attention in transformers lets each token focus on other tokens through attention weights, capturing long-range dependencies without recurrent networks."}
{"input":"Cosine similarity is a metric that measures how similar two vectors are by looking at the angle between them rather than their length. It is widely used in NLP to compare document vectors and embeddings because it focuses on orientation, not magnitude.","output":"Cosine similarity measures the similarity of two vectors by the angle between them and is commonly used to compare text embeddings because it ignores vector magnitude."}
{"input":"A transformer is a neural network architecture that relies on self-attention instead of recurrence or convolutions to process sequences. It can consider all positions at once and uses stacked encoder and decoder layers, which makes it effective for many NLP tasks.","output":"Transformers are neural networks that process sequences using self-attention instead of recurrence, with stacked encoder and decoder layers that work very well for NLP tasks."}
{"input":"Embeddings are vector representations of text that place words or sentences in a continuous space where similar meanings are close together. They are the foundation of semantic search, recommendation systems and many Retrieval-Augmented Generation pipelines.","output":"Embeddings represent text as vectors where similar meanings are close together, forming the basis for semantic search, recommendations and many RAG pipelines."}
{"input":"Retrieval-Augmented Generation, or RAG, combines a retriever that fetches relevant documents with a generator model like GPT. Instead of relying only on its internal parameters, the model uses retrieved context to answer questions more accurately and with up-to-date information.","output":"RAG combines a retriever that brings relevant documents with a generator model, so the model answers using external context instead of only its internal knowledge."}